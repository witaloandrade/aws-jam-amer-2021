{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet Datasets\n",
    "\n",
    "Wrangler has 3 different write modes to store Parquet Datasets on Amazon S3.\n",
    "\n",
    "- **append** (Default)\n",
    "\n",
    "    Only adds new files without any delete.\n",
    "    \n",
    "- **overwrite**\n",
    "\n",
    "    Deletes everything in the target directory and then add new files.\n",
    "    \n",
    "- **overwrite_partitions** (Partition Upsert)\n",
    "\n",
    "    Only deletes the paths of partitions that should be updated and then writes the new partitions files. It's like a \"partition Upsert\".\n",
    "\n",
    "Further resources:\n",
    "- [Official Documentation](https://aws-data-wrangler.readthedocs.io/en/latest/)\n",
    "- [Official Repository](https://github.com/awslabs/aws-data-wrangler)\n",
    "- [Official Tutorials](https://github.com/awslabs/aws-data-wrangler/tree/master/tutorials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "import json\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm = boto3.client(\"ssm\")\n",
    "s3_bucket_param = json.loads(ssm.get_parameter(Name=\"/jam/notebook/bucket\", WithDecryption=True)['Parameter']['Value'])\n",
    "bucket = s3_bucket_param['s3-bucket-name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define s3 path for creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"s3://{bucket}/awswrangler/parquet_dataset/\"\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"id\": [1, 2],\n",
    "    \"value\": [\"foo\", \"boo\"],\n",
    "    \"date\": [date(2020, 1, 1), date(2020, 1, 2)]\n",
    "})\n",
    "\n",
    "wr.s3.to_parquet(\n",
    "    df=df,\n",
    "    path=path,\n",
    "    dataset=True,\n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "df = wr.s3.read_parquet(path, dataset=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.shape == (2, 3)\n",
    "assert df.id.sum() == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate s3 path below to see 1 file is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"id\": [3],\n",
    "    \"value\": [\"bar\"],\n",
    "    \"date\": [date(2020, 1, 3)]\n",
    "})\n",
    "\n",
    "wr.s3.to_parquet(\n",
    "    df=df,\n",
    "    path=path,\n",
    "    dataset=True,\n",
    "    mode=\"append\"\n",
    ")\n",
    "\n",
    "df = wr.s3.read_parquet(path, dataset=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.shape == (3, 3)\n",
    "assert df.id.sum() == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate s3 path below to see 2 files are present (1 new file was added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overwriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.s3.to_parquet(\n",
    "    df=df,\n",
    "    path=path,\n",
    "    dataset=True,\n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "df = wr.s3.read_parquet(path, dataset=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.shape == (3, 3)\n",
    "assert df.id.sum() == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate s3 path below to see 1 file is present (previous files were overwritten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a **Partitoned** Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"id\": [1, 2],\n",
    "    \"value\": [\"foo\", \"boo\"],\n",
    "    \"date\": [date(2020, 1, 1), date(2020, 1, 2)]\n",
    "})\n",
    "\n",
    "wr.s3.to_parquet(\n",
    "    df=df,\n",
    "    path=path,\n",
    "    dataset=True,\n",
    "    mode=\"overwrite\",\n",
    "    partition_cols=[\"date\"]\n",
    ")\n",
    "\n",
    "df = wr.s3.read_parquet(path, dataset=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.shape == (2, 3)\n",
    "assert df.id.sum() == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate s3 path below to see 2 new folders are added and there are files inside each (previous file was overwritten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upserting partitions (overwrite_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"id\": [2, 3],\n",
    "    \"value\": [\"xoo\", \"bar\"],\n",
    "    \"date\": [date(2020, 1, 2), date(2020, 1, 3)]\n",
    "})\n",
    "\n",
    "wr.s3.to_parquet(\n",
    "    df=df,\n",
    "    path=path,\n",
    "    dataset=True,\n",
    "    mode=\"overwrite_partitions\",\n",
    "    partition_cols=[\"date\"]\n",
    ")\n",
    "\n",
    "df = wr.s3.read_parquet(path, dataset=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.shape == (3, 3)\n",
    "assert df.id.sum() == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate s3 path below to see 1 new folder is added (i.e. there are 3 folders) and there are files inside each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}